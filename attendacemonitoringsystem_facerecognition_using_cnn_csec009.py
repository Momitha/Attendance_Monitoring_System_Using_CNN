# -*- coding: utf-8 -*-
"""AttendaceMonitoringSystem_FaceRecognition_Using_CNN_CSEC009.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/163V5D6cFbYSzXV-ukoPdnGj04rdAnf0V
"""

#Attendance_Monitoring_System_Using_CNN

"""**NEW USER REGISTRATION**"""

newfolder_name='siri-160618733161' # enter the folder name to create example :kohli-18(studentname-rollnum)
!mkdir attendance_monitoring/$newfolder_name

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

! mv photo.jpg photo7.jpg

# ! mv photo1.jpg attendance_monitoring/$newfolder_name # move the recent photo to new folder

! mv photo*.jpg attendance_monitoring/$newfolder_name # move the recent photo to new folder

"""**UPLOAD THE DATASET**"""

from google.colab import files
files.upload() # upload the pictures.zip

!unzip attendance_monitoring.zip

import matplotlib.pyplot as plt 
import pandas as pd
import seaborn as sns
import numpy as np
import cv2
import os
import warnings
warnings.filterwarnings('ignore')
from google.colab.patches import cv2_imshow
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import PIL
import io
import html
import time
import pytz
from datetime import datetime

test_file='attendance_monitoring/masrath-160618733154/IMG-20191103-WA0058.jpg'
cv2_imshow(cv2.imread(test_file))

"""**CROP THE IMAGE**"""

import cv2
face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')
img=cv2.imread(test_file)
# cv2_imshow(img)
face=face_cascade.detectMultiScale(img,1.3,5)
# print(face)
if face!=():
  # print('1')
  for x,y,w,h in face:
    # cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    # cv2_imshow(img)
    # gray=cv2.imread(test_file)
    crop_img=img[y:y+h,x:x+w,:]
    cv2_imshow(crop_img)
else :
  print('No face detected')

imgs_path='attendance_monitoring/'
class_names=os.listdir(imgs_path)
img_size=128

! ls -lrth attendance_monitoring/masrath*/* | wc -l

"""**DATA AUGMENTATION**

The code will take around 5 minutes to execute
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

for folder in os.listdir(imgs_path):
  path=os.path.join(imgs_path,folder)
  for image in os.listdir(path):
      # print(image)
      img_loc=os.path.join(path,image)
      # print(img_loc)
      # cv2_imshow(cv2.imread(img_loc))



      img = load_img(img_loc)  # this is a PIL image
      x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)
      x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)

      # the .flow() command below generates batches of randomly transformed images
      # and saves the results to the `preview/` directory
      i = 0
      for batch in datagen.flow(x, batch_size=1,
                                save_to_dir=path, save_prefix='augmented', save_format='jpeg'):
          i += 1
          if i > 3: #
              break  # otherwise the generator would loop indefinitely

! ls -lrth attendance_monitoring/masrath* | wc -l

class_names

face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')
def data_prep(data_dir):
  img_data = [] 
  labels=[]
  for label in class_names: 
        path = os.path.join(data_dir, label)
        class_num = class_names.index(label)
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)
                face=face_cascade.detectMultiScale(img_arr,1.3,5)
                if face!=():
                  # print('face detectoed')
                  for x,y,w,h in face:
                    # cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
                    # cv2_imshow(img)
                    # gray=cv2.imread(test_file)
                    crop_img=img_arr[y:y+h,x:x+w,:]
                    # cv2_imshow(crop_img)
               
                resized_arr = cv2.resize(crop_img, (img_size, img_size))
                img_data.append(resized_arr)
                labels.append(class_num)
                
                                
            except Exception as e:
                print(e)
  return np.array(img_data),np.array(labels)

img_data,labels = data_prep(imgs_path)
print(f'the shape of input image data is {img_data.shape}, labels shape is {labels.shape}')

fig=plt.figure(figsize=(6,4))
ax=fig.add_subplot(111)
sns.set_style('dark')
sns.countplot(labels)
plt.title('Images claases distribution')
plt.xlabel('Class ')
plt.ylabel('Count')
ax.set_xticklabels(class_names,rotation=45)

import random
fig,ax=plt.subplots(5,2)
fig.set_size_inches(15,15)
for i in range(5):
    for j in range (2):
        l=random.randint(0,len(img_data))
        ax[i,j].imshow(img_data[l])
        ax[i,j].set_title('Image class : '+str(class_names[labels[l]]))
        
plt.tight_layout()

from tensorflow.keras.applications.xception import preprocess_input
img_data=preprocess_input(img_data)
img_data[0]

from tensorflow.keras.utils import to_categorical
labels=to_categorical(labels)
labels[0]

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(img_data,labels,test_size=0.2,random_state=0,stratify=labels)
print(f'X_train size is {X_train.shape}, X_test shape is {X_test.shape}')

from tensorflow.keras.applications.xception import Xception
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
# x_train=preprocess_input(x_train)
xception=Xception(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))
for layer in xception.layers[:-1]:
  layer.trainable=False

model=Sequential()
model.add(xception)
model.add(Flatten())
model.add(Dense(len(class_names),activation='softmax'))

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

model.summary()

from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping
learning_rate=ReduceLROnPlateau(monitor='val_accuracy',
                                           patience=3,
                                           verbose=1,
                                           factor=0.5,
                                           min_lr=0.0001)
cb_early_stop=EarlyStopping(monitor = 'val_loss', patience = 2)
callbacks_list=[learning_rate,cb_early_stop]

history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=4,batch_size=64,callbacks=[callbacks_list])

plt.figure(figsize=[8,6])
plt.plot(history.history['accuracy'],'r',linewidth=2.0)
plt.plot(history.history['val_accuracy'],'b',linewidth=2.0)
plt.legend(['Training accuracy', 'Validation accuracy'],fontsize=18)
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('accuracy',fontsize=16)
plt.title('val accuracy',fontsize=16)

test1='attendance_monitoring/masrath-160618733154/augmented_0_2525.jpeg'  # enter file name
x1=[]
img_1 = cv2.imread(test1, cv2.IMREAD_COLOR)
face=face_cascade.detectMultiScale(img_1,1.3,5)
    # print(face)
if face!=():
      print('face detected')
      for x,y,w,h in face:
        cv2.rectangle(img_1,(x,y),(x+w,y+h),(255,0,0),2)
        # cv2_imshow(img)
        # gray=cv2.imread(test_file)
        crop_img=img_1[y:y+h,x:x+w,:]
        # cv2_imshow(crop_img)
        # cv2.putText(frame, label+"  "+str(score*100)+'%', (x, y), font, 1, (0, 255, 0), 2)
        # print(crop_img.shape)
        resized_img = cv2.resize(crop_img, (img_size, img_size))
        # print(crop_img.shape)
        img_preproc=preprocess_input(resized_img)
        img_1 = img_preproc.reshape(-1, img_size, img_size, 3)
        # print(img_1.shape)
        #Calling the predict method on model to predict 'me' on the image
        pred = model.predict(img_1)
        # label=np.argmax(prediction)
        score = np.max(pred)
        # label=get_labels(label)
        pred1=np.argmax(pred,axis=1) # for predicting class
        # print(class_names[pred1[0]])
        label=class_names[pred1[0]]
        print('Predicted person is :',label)
else:
  print('No face detected')

pred_results=pd.DataFrame(data=pred,columns=class_names)
# pred_results.head()
import seaborn as sns
# sns.set_theme(style="darkgrid")
ax=sns.barplot(data=pred_results)
ax.set_xticklabels(class_names,rotation=45)
plt.show()

"""code below is the alternative to use webcam in colab as we have to access google api to do that .so we have to capture image and then apply model to get predictions"""

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

"""enter the photo name below to check the prediction"""

test1='photo.jpg'  # enter file name       ex: photo.jpg
x1=[]
img_1 = cv2.imread(test1, cv2.IMREAD_COLOR)
face=face_cascade.detectMultiScale(img_1,1.3,5)
    # print(face)
if face!=():
      print('face detected')
      for x,y,w,h in face:
        cv2.rectangle(img_1,(x,y),(x+w,y+h),(255,0,0),2)
        # cv2_imshow(img)
        # gray=cv2.imread(test_file)
        crop_img=img_1[y:y+h,x:x+w,:]
        # cv2_imshow(crop_img)
        # cv2.putText(frame, label+"  "+str(score*100)+'%', (x, y), font, 1, (0, 255, 0), 2)
        # print(crop_img.shape)
        resized_img = cv2.resize(crop_img, (img_size, img_size))
        # print(crop_img.shape)
        img_preproc=preprocess_input(resized_img)
        img_1 = img_preproc.reshape(-1, img_size, img_size, 3)
        # print(img_1.shape)
        #Calling the predict method on model to predict 'me' on the image
        pred = model.predict(img_1)
        # label=np.argmax(prediction)
        score = np.max(pred)
        # label=get_labels(label)
        pred1=np.argmax(pred,axis=1) # for predicting class
        # print(class_names[pred1[0]])
        label=class_names[pred1[0]]
        print('Predicted person is :',label)
else:
  print('No face detected')

pred_results=pd.DataFrame(data=pred,columns=class_names)
# pred_results.head()
import seaborn as sns
# sns.set_theme(style="darkgrid")
ax=sns.barplot(data=pred_results)
ax.set_xticklabels(class_names,rotation=45)
plt.show()

"""**Real time face recognition**"""

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;
    
    var pendingResolve = null;
    var shutdown = false;
    
    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }
    
    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }
    
    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);
      
      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);
           
      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);
      
      const instruction = document.createElement('div');
      instruction.innerHTML = 
          '<span style="color: red; font-weight: bold;">' +
          'When finished, click here or on the video to stop this demo</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };
      
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);
      
      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();
      
      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }
            
      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }
      
      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;
      
      return {'create': preShow - preCreate, 
              'show': preCapture - preShow, 
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)
  
def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img


# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes

from tensorflow.keras.applications.xception import preprocess_input
font = cv2.FONT_HERSHEY_SIMPLEX
# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')
bbox = ''
count = 0 
face_data=[]
dt_data=[]
roll_num=[]
while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    # convert JS response to OpenCV Image
    img = js_to_image(js_reply["img"])

    # img=cv2.imread(test_file)
    # cv2_imshow(img)
    # grayscale image for face detection
    # gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    # create transparent overlay for bounding box
    # bbox_array = np.zeros([480,640,4], dtype=np.uint8)

    face=face_cascade.detectMultiScale(img,1.3,5)
    # print(face)
    if face!=():
      print('face detected')
      for x,y,w,h in face:
        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
        # cv2_imshow(img)
        # gray=cv2.imread(test_file)
        crop_img=img[y:y+h,x:x+w,:]
        # cv2_imshow(crop_img)
        # cv2.putText(frame, label+"  "+str(score*100)+'%', (x, y), font, 1, (0, 255, 0), 2)
        # print(crop_img.shape)
        resized_img = cv2.resize(crop_img, (img_size, img_size))
        # print(crop_img.shape)
        img_preproc=preprocess_input(resized_img)
        img_1 = img_preproc.reshape(-1, img_size, img_size, 3)
        # print(img_1.shape)
        #Calling the predict method on model to predict 'me' on the image
        pred = model.predict(img_1)
        # label=np.argmax(prediction)
        score = np.max(pred)
        # label=get_labels(label)
        pred1=np.argmax(pred,axis=1) # for predicting class
        # print(class_names[pred1[0]])
        label=class_names[pred1[0]]
        if label not in face_data:
          face_data.append(label.split('-')[0])
          roll_num.append(label.split('-')[1])
          # curr = datetime.now()
          IST = pytz.timezone('Asia/Kolkata')
          curr=datetime.now(IST)
          dt = curr.strftime("%d/%m/%Y, %H:%M:%S")
          dt_data.append(dt)
      
        # label='hi'
        # cv2.putText(img,pred1[0],(x,y),font, 1, (0, 255, 0), 2)
        # cv2.putText(img, label, (x, y), 1, (0, 255, 0), 2)
        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
        cv2.putText(img, label+"  "+str(score*100)+'%', (x, y), font, 1, (0, 255, 0), 2)
        cv2_imshow(img)
        # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)
        

      # bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
      # convert overlay of bbox into bytes
      # bbox_bytes = bbox_to_bytes(bbox_array)
      # update bbox so next frame gets new overlay
      # bbox = bbox_bytes
    else :
      print('No face detected')

results=pd.DataFrame(data={'roll number':roll_num,'Student Name':face_data,'Date':dt_data})
current_date=(datetime.now()).strftime('%d-%m-%y')
results.to_csv('Attendance_report_'+current_date+'.csv',index=False)
print(results.head())

!ls -lrth # list of files

files.download('Attendance_report_28-03-22.csv') # enter file name to download